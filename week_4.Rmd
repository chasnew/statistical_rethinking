---
title: "Week 4"
output: html_notebook
---

```{r message=FALSE}
library(tidyverse)
library(rethinking)
library(MASS)
library(dagitty)
```

```{r}
# Overfitting
sppnames <- c( "afarensis","africanus","habilis","boisei","rudolfensis","ergaster","sapiens")
brainvolcc <- c( 438 , 452 , 612, 521, 752, 871, 1350 ) # brain volume (cc)
masskg <- c( 37.0 , 35.5 , 34.5 , 41.5 , 55.5 , 61.0 , 53.5 ) # body mass (kg)
d <- data.frame( species=sppnames , brain=brainvolcc , mass=masskg )

d$mass_std <- (d$mass - mean(d$mass))/sd(d$mass) # standardize mass
d$brain_std <- d$brain / max(d$brain) # scale brain volume

# highly flexible model
m7.1 <- quap(
  alist(
    brain_std ~ dnorm( mu , exp(log_sigma) ),
    mu <- a + b*mass_std,
    a ~ dnorm( 0.5 , 1 ),
    b ~ dnorm( 0 , 10 ),
    log_sigma ~ dnorm( 0 , 1 )
  ), data=d )

# calculate r-squared of m7.1
set.seed(12)
s <- sim( m7.1 )
r <- apply(s,2,mean) - d$brain_std
resid_var <- var2(r)
outcome_var <- var2( d$brain_std )
1 - resid_var/outcome_var

# R2 function
R2_is_bad <- function( quap_fit ) {
  s <- sim( quap_fit , refresh=0 )
  r <- apply(s,2,mean) - d$brain_std
  1 - var2(r)/var2(d$brain_std)
}

# Other more complex models
m7.2 <- quap(
  alist(
    brain_std ~ dnorm( mu , exp(log_sigma) ),
    mu <- a + b[1]*mass_std + b[2]*mass_std^2,
    a ~ dnorm( 0.5 , 1 ),
    b ~ dnorm( 0 , 10 ),
    log_sigma ~ dnorm( 0 , 1 )
  ), data=d , start=list(b=rep(0,2)) )

m7.3 <- quap(
  alist(
    brain_std ~ dnorm( mu , exp(log_sigma) ),
    mu <- a + b[1]*mass_std + b[2]*mass_std^2 +
      b[3]*mass_std^3,
    a ~ dnorm( 0.5 , 1 ),
    b ~ dnorm( 0 , 10 ),
    log_sigma ~ dnorm( 0 , 1 )
  ), data=d , start=list(b=rep(0,3)) )

m7.4 <- quap(
  alist(
    brain_std ~ dnorm( mu , exp(log_sigma) ),
    mu <- a + b[1]*mass_std + b[2]*mass_std^2 +
      b[3]*mass_std^3 + b[4]*mass_std^4,
    a ~ dnorm( 0.5 , 1 ),
    b ~ dnorm( 0 , 10 ),
    log_sigma ~ dnorm( 0 , 1 )
  ), data=d , start=list(b=rep(0,4)) )

m7.5 <- quap(
  alist(
    brain_std ~ dnorm( mu , exp(log_sigma) ),
    mu <- a + b[1]*mass_std + b[2]*mass_std^2 +
      b[3]*mass_std^3 + b[4]*mass_std^4 +
      b[5]*mass_std^5,
    a ~ dnorm( 0.5 , 1 ),
    b ~ dnorm( 0 , 10 ),
    log_sigma ~ dnorm( 0 , 1 )
  ), data=d , start=list(b=rep(0,5)) )

m7.6 <- quap(
  alist(
    brain_std ~ dnorm( mu , 0.001 ),
    mu <- a + b[1]*mass_std + b[2]*mass_std^2 +
      b[3]*mass_std^3 + b[4]*mass_std^4 +
      b[5]*mass_std^5 + b[6]*mass_std^6,
    a ~ dnorm( 0.5 , 1 ),
    b ~ dnorm( 0 , 10 )
  ), data=d , start=list(b=rep(0,6)) )

# plot prediction results

# post <- extract.samples(m7.1)
# mass_seq <- seq( from=min(d$mass_std) , to=max(d$mass_std) , length.out=100 )
# l <- link( m7.1 , data=list( mass_std=mass_seq ) )
# mu <- apply( l , 2 , mean )
# ci <- apply( l , 2 , PI )
# plot( brain_std ~ mass_std , data=d )
# lines( mass_seq , mu )
# shade( ci , mass_seq )

for (m in c(m7.1, m7.2, m7.3, m7.4, m7.5, m7.6)) brain_plot(m)
```

```{r}
# Uncertainty and entropy
set.seed(1)

# LOG-POINTWISE-PREDICTIVE-DENSITY
# lppd( m7.1 , n=1e4 )
logprob <- sim( m7.1 , ll=TRUE , n=1e4 )
n <- ncol(logprob)
ns <- nrow(logprob)
f <- function( i ) log_sum_exp( logprob[,i] ) - log(ns)
( lppd <- sapply( 1:n , f ) )

# compute lppd for all the models in the previous section
set.seed(1)
sapply( list(m7.1,m7.2,m7.3,m7.4,m7.5,m7.6) , function(m) sum(lppd(m)) )

# Deviance as a performance measure of in-sample and out-of-sample predictions
# Figure 7.6 thought experiment
# N <- 20
# kseq <- 1:5
# dev <- sapply( kseq , function(k) {
#   print(k);
#   # modify sim_train_test to simulate different scenarios
#   r <- mcreplicate( 1e4 , sim_train_test( N=N, k=k ) , mc.cores=4 )
#   c( mean(r[1,]) , mean(r[2,]) , sd(r[1,]) , sd(r[2,]) )
# } )
# 
# plot( 1:5 , dev[1,] , ylim=c( min(dev[1:2,])-5 , max(dev[1:2,])+10 ) ,
#       xlim=c(1,5.1) , xlab="number of parameters" , ylab="deviance" ,
#       pch=16 , col=rangi2 )
# mtext( concat( "N = ",N ) )
# points( (1:5)+0.1 , dev[2,] )
# for ( i in kseq ) {
#   pts_in <- dev[1,i] + c(-1,+1)*dev[3,i]
#   pts_out <- dev[2,i] + c(-1,+1)*dev[4,i]
#   lines( c(i,i) , pts_in , col=rangi2 )
#   lines( c(i,i)+0.1 , pts_out )
# }


# WAIC calculation

data(cars)
m <- quap(
  alist(
    dist ~ dnorm(mu,sigma),
    mu <- a + b*speed,
    a ~ dnorm(0,100),
    b ~ dnorm(0,10),
    sigma ~ dexp(1)
  ) , data=cars )

set.seed(94)
post <- extract.samples(m,n=1000)

n_samples <- 1000
# log-likelihoods: 50 x 1000 matrix
logprob <- sapply( 1:n_samples ,
                   function(s) {
                     mu <- post$a[s] + post$b[s]*cars$speed
                     dnorm( cars$dist , mu , post$sigma[s] , log=TRUE )
                    } )

n_cases <- nrow(cars)
lppd <- sapply( 1:n_cases , function(i) log_sum_exp(logprob[i,]) - log(n_samples) )
pWAIC <- sapply( 1:n_cases , function(i) var(logprob[i,]) ) # the penalty term
-2*( sum(lppd) - sum(pWAIC) ) # WAIC
# WAIC standard error
waic_vec <- -2*( lppd - pWAIC )
sqrt( n_cases*var(waic_vec) )
```

```{r}
# Model Comparison

set.seed(71)

# number of plants
N <- 100

# simulate initial heights
h0 <- rnorm(N,10,2)

# assign treatments and simulate fungus and growth
treatment <- rep( 0:1 , each=N/2 )
fungus <- rbinom( N , size=1 , prob=0.5 - treatment*0.4 )
h1 <- h0 + rnorm(N, 5 - 3*fungus)

# compose a clean data frame
d <- data.frame( h0=h0 , h1=h1 , treatment=treatment , fungus=fungus )
sim_p <- rlnorm( 1e4 , 0 , 0.25 ) # log-normal prior

# first height proportion model
m6.6 <- quap(
  alist(
    h1 ~ dnorm( mu , sigma ),
    mu <- h0*p,
    p ~ dlnorm( 0 , 0.25 ),
    sigma ~ dexp( 1 )
    ), data=d )

# modeling proportion parameter as a function of predictor variables (treatment & fungus)
# the treatment effect gets blocked by fungus
m6.7 <- quap(
  alist(
    h1 ~ dnorm( mu , sigma ),
    mu <- h0 * p,
    p <- a + bt*treatment + bf*fungus,
    a ~ dlnorm( 0 , 0.2 ) ,
    bt ~ dnorm( 0 , 0.5 ),
    bf ~ dnorm( 0 , 0.5 ),
    sigma ~ dexp( 1 )
  ), data=d )

# correct model to the question about the treatment effect
m6.8 <- quap(
  alist(
    h1 ~ dnorm( mu , sigma ),
    mu <- h0 * p,
    p <- a + bt*treatment,
    a ~ dlnorm( 0 , 0.2 ),
    bt ~ dnorm( 0 , 0.5 ),
    sigma ~ dexp( 1 )
  ), data=d )

# Calculate WAIC scores to compare models
set.seed(11)
WAIC( m6.7 )

set.seed(77)
compare( m6.6 , m6.7 , m6.8 , func=WAIC )
# compare( m6.6 , m6.7 , m6.8 , func=PSIS ) # calculate PSIS

# quantify whether models are easy to distinguish (dSE)
set.seed(91)
waic_m6.7 <- WAIC( m6.7 , pointwise=TRUE )$WAIC
waic_m6.8 <- WAIC( m6.8 , pointwise=TRUE )$WAIC
n <- length(waic_m6.7)
diff_m6.7_m6.8 <- waic_m6.7 - waic_m6.8
sqrt( n*var( diff_m6.7_m6.8 ) )

plot( compare( m6.6 , m6.7 , m6.8 ) )

# compare the treatment model with the null model
set.seed(92)
waic_m6.6 <- WAIC( m6.6 , pointwise=TRUE )$WAIC
diff_m6.6_m6.8 <- waic_m6.6 - waic_m6.8
sqrt( n*var( diff_m6.6_m6.8 ) )

set.seed(93)
compare( m6.6 , m6.7 , m6.8 )@dSE

# Outliers

# waffle house and marriage rate example
data(WaffleDivorce)
d <- WaffleDivorce
d$A <- standardize( d$MedianAgeMarriage )
d$D <- standardize( d$Divorce )
d$M <- standardize( d$Marriage )

m5.1 <- quap(
  alist(
    D ~ dnorm( mu , sigma ) ,
    mu <- a + bA * A ,
    a ~ dnorm( 0 , 0.2 ) ,
    bA ~ dnorm( 0 , 0.5 ) ,
    sigma ~ dexp( 1 )
  ) , data = d )

m5.2 <- quap(
  alist(
    D ~ dnorm( mu , sigma ) ,
    mu <- a + bM * M ,
    a ~ dnorm( 0 , 0.2 ) ,
    bM ~ dnorm( 0 , 0.5 ) ,
    sigma ~ dexp( 1 )
  ) , data = d )

m5.3 <- quap(
  alist(
    D ~ dnorm( mu , sigma ) ,
    mu <- a + bM*M + bA*A ,
    a ~ dnorm( 0 , 0.2 ) ,
    bM ~ dnorm( 0 , 0.5 ) ,
    bA ~ dnorm( 0 , 0.5 ) ,
    sigma ~ dexp( 1 )
  ) , data = d )

set.seed(24071847)
compare( m5.1 , m5.2 , m5.3 , func=PSIS )

# scrutinize outliers
set.seed(24071847)
PSIS_m5.3 <- PSIS(m5.3,pointwise=TRUE)

set.seed(24071847)
WAIC_m5.3 <- WAIC(m5.3,pointwise=TRUE)

# figure 7.10
plot( PSIS_m5.3$k , WAIC_m5.3$penalty , xlab="PSIS Pareto k" ,
      ylab="WAIC penalty" , col=rangi2 , lwd=2 )

# robust regression (using Student-T distribution to replace Gaussian)
m5.3t <- quap(
  alist(
    D ~ dstudent( 2 , mu , sigma ) , # nu = 2
    mu <- a + bM*M + bA*A ,
    a ~ dnorm( 0 , 0.2 ) ,
    bM ~ dnorm( 0 , 0.5 ) ,
    bA ~ dnorm( 0 , 0.5 ) ,
    sigma ~ dexp( 1 )
  ) , data = d )

# no longer have unreliable PSIS
PSIS_m5.3t <- PSIS(m5.3t,pointwise=TRUE)
ggplot(data = PSIS_m5.3t, aes(x = k)) +
  geom_histogram(bins = 100)

# the bA coefficient gets further away from 0 because the outlier is less influential
precis(m5.3)
precis(m5.3t)
```

